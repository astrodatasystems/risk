{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOAA Storm Events 2023 - Data Exploration\n",
    "\n",
    "**Phase 2 - Data Collection and Initial Analysis**\n",
    "\n",
    "This notebook contains our initial exploration of the NOAA Storm Events 2023 dataset, which forms the foundation of our AmFam Risk Model project.\n",
    "\n",
    "## Key Findings Summary\n",
    "\n",
    "- **Dataset Size**: 75,593 storm events across 51 columns\n",
    "- **Coverage**: Complete 2023 calendar year\n",
    "- **Geographic**: County-level data (CZ_TYPE: C = County, Z = Zone)\n",
    "- **Damage Format**: String format like '25.00K', '1.00M', '100.00M' requiring parsing\n",
    "- **Event Types**: 51 different weather event types, dominated by Thunderstorm Wind and Hail\n",
    "\n",
    "## Business Context\n",
    "\n",
    "This dataset will feed into our:\n",
    "1. **Risk Tier Classifier**: Predicting Low/Moderate/High/Extreme risk levels by county\n",
    "2. **Damage Regressor**: Estimating expected annual property damage ($) by county\n",
    "3. **Feature Engineering**: Aggregating event-level data to county-level statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the data\n",
    "data_path = Path('../data/01_raw/noaa/StormEvents_details_2023.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset loaded: {len(df):,} rows x {len(df.columns)} columns\")\n",
    "print(f\"File size: {data_path.stat().st_size / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset info\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nShape: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n",
    "print(f\"Date range: {df['YEAR'].min()} (all {df['YEAR'].nunique()} year(s))\")\n",
    "\n",
    "# Show all column names\n",
    "print(f\"\\nColumn Names ({len(df.columns)} total):\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event Types Analysis\n",
    "print(\"EVENT TYPES - Critical for Risk Classification\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "event_counts = df['EVENT_TYPE'].value_counts()\n",
    "print(f\"Total unique event types: {len(event_counts)}\")\n",
    "print(f\"\\nTop 15 Most Frequent Event Types:\")\n",
    "print(event_counts.head(15).to_string())\n",
    "\n",
    "# Calculate percentage of top events\n",
    "top_5_pct = (event_counts.head(5).sum() / len(df)) * 100\n",
    "print(f\"\\nTop 5 event types represent {top_5_pct:.1f}% of all events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# County Zone Types\n",
    "print(\"COUNTY/ZONE TYPES (CZ_TYPE)\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "cz_counts = df['CZ_TYPE'].value_counts()\n",
    "print(cz_counts.to_string())\n",
    "print(f\"\\nC = County, Z = Zone\")\n",
    "print(f\"County events: {cz_counts['C']:,} ({cz_counts['C']/len(df)*100:.1f}%)\")\n",
    "print(f\"Zone events: {cz_counts['Z']:,} ({cz_counts['Z']/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic analysis\n",
    "print(\"GEOGRAPHIC DISTRIBUTION\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "state_counts = df['STATE'].value_counts()\n",
    "print(f\"States covered: {len(state_counts)}\")\n",
    "print(f\"\\nTop 10 States by Event Count:\")\n",
    "print(state_counts.head(10).to_string())\n",
    "\n",
    "print(f\"\\nUnique counties/zones: {df['CZ_NAME'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Damage Analysis - Critical for AmFam Business Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property damage analysis\n",
    "print(\"PROPERTY DAMAGE ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "print(f\"Total events: {len(df):,}\")\n",
    "print(f\"Events with property damage data: {df['DAMAGE_PROPERTY'].count():,}\")\n",
    "print(f\"Events with null property damage: {df['DAMAGE_PROPERTY'].isnull().sum():,}\")\n",
    "print(f\"Coverage: {df['DAMAGE_PROPERTY'].count()/len(df)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nUnique damage values: {df['DAMAGE_PROPERTY'].nunique():,}\")\n",
    "\n",
    "# Show damage format examples\n",
    "print(f\"\\nDAMAGE FORMAT EXAMPLES (Need Parsing):\")\n",
    "damage_examples = ['0.00K', '1.00K', '5.00K', '25.00K', '100.00K', '500.00K', '1.00M', '5.00M', '100.00M']\n",
    "found_examples = [ex for ex in damage_examples if ex in df['DAMAGE_PROPERTY'].values]\n",
    "for i, ex in enumerate(found_examples, 1):\n",
    "    print(f\"{i:2}. '{ex}'\")\n",
    "\n",
    "# Most common values\n",
    "print(f\"\\nMost Frequent Damage Values:\")\n",
    "print(df['DAMAGE_PROPERTY'].value_counts().head(10).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing data analysis\n",
    "print(\"DATA QUALITY - NULL VALUE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "null_counts = df.isnull().sum()\n",
    "null_pct = (null_counts / len(df)) * 100\n",
    "\n",
    "# Only show columns with missing data\n",
    "missing_data = pd.DataFrame({\n",
    "    'Column': null_counts[null_counts > 0].index,\n",
    "    'Missing_Count': null_counts[null_counts > 0].values,\n",
    "    'Missing_Percentage': null_pct[null_counts > 0].values\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(f\"Columns with missing data: {len(missing_data)} out of {len(df.columns)}\")\n",
    "print(f\"\\nTop 15 columns by missing values:\")\n",
    "print(missing_data.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key columns for risk modeling - completeness check\n",
    "print(\"KEY COLUMNS FOR RISK MODELING\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "key_columns = ['STATE', 'CZ_TYPE', 'CZ_FIPS', 'CZ_NAME', 'EVENT_TYPE', \n",
    "               'DAMAGE_PROPERTY', 'DAMAGE_CROPS', 'INJURIES_DIRECT', \n",
    "               'DEATHS_DIRECT', 'BEGIN_LAT', 'BEGIN_LON']\n",
    "\n",
    "for col in key_columns:\n",
    "    if col in df.columns:\n",
    "        null_count = df[col].isnull().sum()\n",
    "        null_pct = (null_count / len(df)) * 100\n",
    "        print(f\"{col:20}: {null_count:6,} nulls ({null_pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data for Understanding Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first few rows with key columns for understanding\n",
    "key_display_cols = ['STATE', 'CZ_NAME', 'EVENT_TYPE', 'DAMAGE_PROPERTY', \n",
    "                   'BEGIN_DATE_TIME', 'EVENT_NARRATIVE']\n",
    "\n",
    "print(\"SAMPLE DATA - Key Columns\")\n",
    "print(\"=\" * 30)\n",
    "display_df = df[key_display_cols].head()\n",
    "for idx, row in display_df.iterrows():\n",
    "    print(f\"\\nRow {idx + 1}:\")\n",
    "    for col in key_display_cols:\n",
    "        value = str(row[col])[:100] + \"...\" if len(str(row[col])) > 100 else str(row[col])\n",
    "        print(f\"  {col:15}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps for Silver Layer\n",
    "\n",
    "Based on this exploration, our Bronze → Silver transformation pipeline needs to:\n",
    "\n",
    "### Data Cleaning Requirements\n",
    "1. **Parse damage values**: Convert '25.00K' → 25000, '1.00M' → 1000000\n",
    "2. **Standardize event types**: Group similar events (e.g., various wind events)\n",
    "3. **Handle missing coordinates**: Impute or flag events without lat/lon\n",
    "4. **Date standardization**: Ensure consistent datetime formats\n",
    "5. **County consolidation**: Focus on CZ_TYPE='C' for county-level analysis\n",
    "\n",
    "### Feature Engineering for Gold Layer\n",
    "1. **County-level aggregation**: Sum damages, count events by type per county\n",
    "2. **Risk indicators**: Create severity scores based on damage/injury/death patterns\n",
    "3. **Temporal features**: Seasonality, trends, event frequency patterns\n",
    "4. **Geographic features**: Regional risk patterns, neighboring county effects\n",
    "\n",
    "### Model Input Preparation\n",
    "- Target variables: Risk tier (Low/Moderate/High/Extreme) + Expected damage ($)\n",
    "- Features: County weather patterns, historical damage, event frequency by type\n",
    "- Granularity: County-year level for training data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk",
   "language": "python",
   "name": "risk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}